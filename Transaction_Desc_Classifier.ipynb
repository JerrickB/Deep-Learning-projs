{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KDGEskyb7K7C"
   },
   "source": [
    "To Do:\n",
    "    Fix save dataset save position position\n",
    "    \n",
    "    Separate cleaned data save(possibly move to helpfun?)\n",
    "    \n",
    "    Clean desc strings\n",
    "    \n",
    "    Split desc strings into single objects\n",
    "    \n",
    "    Make bag-of words\n",
    "    \n",
    "    Decide network architechure \n",
    "    \n",
    "    Make network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "rIDINnSG7K7H",
    "outputId": "9101936e-c13b-4d04-a536-86bb2d43d62c"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Purpose: Classify transactions for budget record keeping\n",
    "Resources: http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/\n",
    "            http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "            https://www.dotnetperls.com/translate-python\n",
    "            https://www.kaggle.com/c/word2vec-nlp-tutorial#part-1-for-beginners-bag-of-words\n",
    "            http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/\n",
    "Strategy:\n",
    "    Use basic CNN w/ bag-o-words word-vectorization to classify transaction entries based on bank statement description and pre-selected categories\n",
    "\n",
    "Data: Raw CC transaction data (4/14/17 - 10/11/17) hand stripped and categorized from monthly base statements\n",
    " Raw(Call='dataset'):   \n",
    "    Entries       200\n",
    "    Cols          date, desc, blank, amount, c1, c2\n",
    "    Classes       c2             10:\n",
    "                                    A  -  23  Gas\n",
    "                                    B  -  6   Bill\n",
    "                                    B1 -  1   One-time bill\n",
    "                                    C  -  4   Clothes\n",
    "                                    D  -  18  R&D\n",
    "                                    G  -  24  Grocerry\n",
    "                                    M  -  28  MISC\n",
    "                                    R  -  71  Restaurant\n",
    "                                    V  -  13  Vidya\n",
    " Clean('df'):\n",
    "    Entries       196\n",
    "    Features      desc, c2       \n",
    "    Classes       c2             8:\n",
    "                                    A  -  23  Gas\n",
    "                                    B  -  6   Bill\n",
    "                                    D  -  18  R&D\n",
    "                                    G  -  24  Grocerry\n",
    "                                    M  -  28  MISC\n",
    "                                    R  -  71  Restaurant\n",
    "                                    V  -  13  Vidya\n",
    "Python      v3.6.3\n",
    "helperFun   v0.0.1\n",
    "pandas      v0.22.0\n",
    "re          v2.2.1\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "UAJS4jcL7K7q"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "#clean strings\n",
    "def c_str(strings, clean_list = []):\n",
    "    '''\n",
    "    Purpose: Clean string of punct and unwanted words, then split into lists\n",
    "    \n",
    "    :type text str\n",
    "    :type clean_list list\n",
    "    ********\n",
    "    clean_list: optional arg\n",
    "    Accepts passed list of string objs to be removed from string\n",
    "    '''\n",
    "    csstr = []\n",
    "    dDesc = strings\n",
    "    dDesc = dDesc.str.lower()\n",
    "    replace_re = '|'.join([re.escape(s) for s in clean_list] + [r'\\s+', r'[0-9]+'])\n",
    "    for idx, str in enumerate(dDesc):\n",
    "        cstr = []\n",
    "        str = re.sub(r'\\W+', ' ', str)\n",
    "        if str.startswith('uber'):\n",
    "            str = 'uber'\n",
    "        str = str.split()\n",
    "        for word in str:\n",
    "            cstr += [re.sub(replace_re, '', word)]\n",
    "        str = cstr\n",
    "        str = ' '.join([word for word in str if word])\n",
    "        dDesc[idx] = str\n",
    "    return dDesc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "O7NJcGd-7K7g"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R    175\n",
       "A     69\n",
       "D     51\n",
       "G     45\n",
       "B     43\n",
       "S     41\n",
       "M     38\n",
       "V     29\n",
       "Name: c2, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####-------\n",
    "#New transactions\n",
    "\n",
    "df = pd.read_csv('./data/Budget - Sheet14.csv')\n",
    "clean_list = ['phoenix','avondale','tempe', 'help', 'glendale','az','ny']\n",
    "df.loc[df.c2 == 'B1','c2'] = 'B'\n",
    "\n",
    "df.desc = c_str(df.desc, clean_list)\n",
    "df.to_pickle('./data/Trans_04-27.pkl')\n",
    "df.c2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd_classes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-6273764c2f77>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'price'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mclss\u001b[0m \u001b[1;32min\u001b[0m \u001b[0md_classes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mclss\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'R'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc2\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mclss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdesc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'd_classes' is not defined"
     ]
    }
   ],
   "source": [
    "tdf = df.copy().drop('price',axis=1)\n",
    "for clss in d_classes:\n",
    "    if clss not in ['R']:\n",
    "        t = tdf.loc[tdf.c2 == clss,:].desc.value_counts()\n",
    "        t = list(zip(t.keys().tolist(),t.tolist()))\n",
    "        targ = t[0][1]\n",
    "        for desc, act in t[1:]:\n",
    "            need = targ - act\n",
    "            if need < targ:\n",
    "                for i in range(need):\n",
    "                    tdf = tdf.append({'desc':desc,'c2':clss},ignore_index=True)\n",
    "tdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating -1.455 for class B\n",
      "Creating -3.6982 for class B\n",
      "Creating -2.987928 for class B\n",
      "Creating -1.99951712 for class B\n",
      "Creating -1.9999806848 for class B\n",
      "Creating -1.999999227392 for class B\n",
      "Creating -1.99999996909568 for class B\n",
      "Creating -1.9999999987638273 for class B\n",
      "Creating -0.9999999999505531 for class B\n",
      "Creating -0.9999999999980221 for class B\n",
      "Creating -0.9999999999999208 for class B\n",
      "Creating -0.9999999999999968 for class B\n",
      "Creating -0.9999999999999999 for class B\n",
      "Creating -1.0 for class B\n",
      "Creating -1.0 for class B\n",
      "Creating -1.0 for class B\n",
      "Creating -1.0 for class B\n",
      "Creating -1.0 for class B\n",
      "Creating -1.0 for class B\n",
      "Creating -1.0 for class B\n",
      "Creating -1.0 for class B\n",
      "Creating -1.0 for class B\n",
      "Creating -1.0 for class B\n",
      "Creating -1.0 for class B\n",
      "Creating -1.0 for class B\n",
      "Creating -22.0 for class S\n",
      "Creating -15.0 for class S\n",
      "Creating -2.0 for class S\n",
      "Creating -1.0 for class S\n",
      "Creating -1.0 for class S\n",
      "Creating -43.0 for class A\n",
      "Creating -5.0 for class A\n",
      "Creating -5.0 for class A\n",
      "Creating -3.0 for class A\n",
      "Creating -3.0 for class A\n",
      "Creating -2.0 for class A\n",
      "Creating -2.0 for class A\n",
      "Creating -1.0 for class A\n",
      "Creating -1.0 for class A\n",
      "Creating -1.0 for class A\n",
      "Creating -1.0 for class A\n",
      "Creating -1.0 for class A\n",
      "Creating -1.0 for class A\n",
      "Creating -20.0 for class R\n",
      "Creating -15.0 for class R\n",
      "Creating -15.0 for class R\n",
      "Creating -12.0 for class R\n",
      "Creating -11.0 for class R\n",
      "Creating -11.0 for class R\n",
      "Creating -10.0 for class R\n",
      "Creating -9.0 for class R\n",
      "Creating -6.0 for class R\n",
      "Creating -5.0 for class R\n",
      "Creating -4.0 for class R\n",
      "Creating -3.0 for class R\n",
      "Creating -3.0 for class R\n",
      "Creating -3.0 for class R\n",
      "Creating -3.0 for class R\n",
      "Creating -2.0 for class R\n",
      "Creating -2.0 for class R\n",
      "Creating -2.0 for class R\n",
      "Creating -2.0 for class R\n",
      "Creating -1.0 for class R\n",
      "Creating -1.0 for class R\n",
      "Creating -1.0 for class R\n",
      "Creating -1.0 for class R\n",
      "Creating -1.0 for class R\n",
      "Creating -1.0 for class R\n",
      "Creating -1.0 for class R\n",
      "Creating -1.0 for class R\n",
      "Creating -1.0 for class R\n",
      "Creating -1.0 for class R\n",
      "Creating -1.0 for class R\n",
      "Creating -1.0 for class R\n",
      "Creating -1.0 for class R\n",
      "Creating -1.0 for class R\n",
      "Creating -1.0 for class R\n",
      "Creating -1.0 for class R\n",
      "Creating -1.0 for class R\n",
      "Creating -1.0 for class R\n",
      "Creating -1.0 for class R\n",
      "Creating -1.0 for class R\n",
      "Creating -1.0 for class R\n",
      "Creating -1.0 for class R\n",
      "Creating -1.0 for class R\n",
      "Creating -1.0 for class R\n",
      "Creating -1.0 for class R\n",
      "Creating -1.0 for class R\n",
      "Creating -1.0 for class R\n",
      "Creating -1.0 for class R\n",
      "Creating -1.0 for class R\n",
      "Creating -1.0 for class R\n",
      "Creating -1.0 for class R\n",
      "Creating -1.0 for class R\n",
      "Creating -1.0 for class R\n",
      "Creating -1.0 for class R\n",
      "Creating -1.0 for class R\n",
      "Creating -1.0 for class R\n",
      "Creating -1.0 for class R\n",
      "Creating -38.0 for class M\n",
      "Creating -20.0 for class G\n",
      "Creating -9.0 for class G\n",
      "Creating -6.0 for class G\n",
      "Creating -4.0 for class G\n",
      "Creating -3.0 for class G\n",
      "Creating -2.0 for class G\n",
      "Creating -1.0 for class G\n",
      "Creating -9.0 for class D\n",
      "Creating -5.0 for class D\n",
      "Creating -4.0 for class D\n",
      "Creating -3.0 for class D\n",
      "Creating -3.0 for class D\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-e20bf75348e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mfraudy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-22-e20bf75348e6>\u001b[0m in \u001b[0;36mfraudy\u001b[1;34m(s_targ, pklpath, blklst)\u001b[0m\n\u001b[0;32m     15\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Creating {} for class {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb_need\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m                         \u001b[0mtdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'desc'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdesc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'c2'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mclss\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mappend\u001b[1;34m(self, other, ignore_index, verify_integrity)\u001b[0m\n\u001b[0;32m   5180\u001b[0m             \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5181\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mequals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcombined_columns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5182\u001b[1;33m                 \u001b[0mself\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcombined_columns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5183\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5184\u001b[0m             \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;33m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mPY2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mreindex\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2933\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'axis'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2934\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'labels'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2935\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2936\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2937\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mAppender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_shared_docs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'reindex_axis'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0m_shared_doc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mreindex\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3021\u001b[0m         \u001b[1;31m# perform the reindex on the axes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3022\u001b[0m         return self._reindex_axes(axes, level, limit, tolerance, method,\n\u001b[1;32m-> 3023\u001b[1;33m                                   fill_value, copy).__finalize__(self)\n\u001b[0m\u001b[0;32m   3024\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3025\u001b[0m     def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_reindex_axes\u001b[1;34m(self, axes, level, limit, tolerance, method, fill_value, copy)\u001b[0m\n\u001b[0;32m   2863\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2864\u001b[0m             frame = frame._reindex_columns(columns, method, copy, level,\n\u001b[1;32m-> 2865\u001b[1;33m                                            fill_value, limit, tolerance)\n\u001b[0m\u001b[0;32m   2866\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2867\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_reindex_columns\u001b[1;34m(self, new_columns, method, copy, level, fill_value, limit, tolerance)\u001b[0m\n\u001b[0;32m   2888\u001b[0m         return self._reindex_with_indexers({1: [new_columns, indexer]},\n\u001b[0;32m   2889\u001b[0m                                            \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2890\u001b[1;33m                                            allow_dups=False)\n\u001b[0m\u001b[0;32m   2891\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2892\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_reindex_multi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_reindex_with_indexers\u001b[1;34m(self, reindexers, fill_value, copy, allow_dups)\u001b[0m\n\u001b[0;32m   3143\u001b[0m                                                 \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3144\u001b[0m                                                 \u001b[0mallow_dups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mallow_dups\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3145\u001b[1;33m                                                 copy=copy)\n\u001b[0m\u001b[0;32m   3146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3147\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mnew_data\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mreindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy)\u001b[0m\n\u001b[0;32m   4128\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4130\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4131\u001b[0m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4132\u001b[0m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(self, deep, mgr)\u001b[0m\n\u001b[0;32m   3662\u001b[0m             \u001b[0mnew_axes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3663\u001b[0m         return self.apply('copy', axes=new_axes, deep=deep,\n\u001b[1;32m-> 3664\u001b[1;33m                           do_integrity_check=False)\n\u001b[0m\u001b[0;32m   3665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3666\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mas_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, f, axes, filter, do_integrity_check, consolidate, **kwargs)\u001b[0m\n\u001b[0;32m   3327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3328\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mgr'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3329\u001b[1;33m             \u001b[0mapplied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3330\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(self, deep, mgr)\u001b[0m\n\u001b[0;32m    734\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 736\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    737\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_block_same_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# def fraudy(s_targ = 2000, pklpath='./data/Trans_04-27.pkl', blklst=[]):\n",
    "#     tdf = pd.read_pickle(pklpath).drop('price',axis=1)\n",
    "#     labels, d_classes = pd.factorize(df.c2)\n",
    "#     s_act = len(tdf)\n",
    "#     s_need =  s_targ - s_act\n",
    "#     if s_need < s_targ:\n",
    "#         b_targ = s_need/len(d_classes)\n",
    "#         for clss in d_classes:\n",
    "#             t = tdf.loc[tdf.c2 == clss,:].desc.value_counts()\n",
    "#             t = list(zip(t.keys().tolist(),t.tolist()))\n",
    "#             for desc, b_act in t:\n",
    "#                 b_targ = b_targ / len(t)\n",
    "#                 b_need = b_targ - b_act\n",
    "# #                 if b_need < b_targ:\n",
    "# #                     print('Creating {} for class {}'.format(b_need, clss))\n",
    "# #                     for i in range(need):\n",
    "# #                         tdf = tdf.append({'desc':desc,'c2':clss},ignore_index=True)\n",
    "#     return tdf\n",
    "\n",
    "# fraudy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "jtOyfgFw7K78",
    "outputId": "d7dd8c06-a11e-49f7-e8fd-85b8d3881257",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 343 samples, validate on 74 samples\n",
      "Epoch 1/50\n",
      "224/343 [==================>...........] - ETA: 2s - loss: 1.9546 - acc: 0.369 - ETA: 1s - loss: 1.8944 - acc: 0.3750"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-620678fe7a81>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;31m#     for e in [50,25,75]:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;31m#         scores.append(('dropout = {}'.format(i),steks(lr=i,epochs=e)))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m \u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msteks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-620678fe7a81>\u001b[0m in \u001b[0;36msteks\u001b[1;34m(d1_nodes, d1_activation, d1_dropout, d2_nodes, d2_activation, d2_dropout, d3_nodes, d3_activation, d3_dropout, d4_nodes, d4_activation, d4_dropout, d5_nodes, d5_activation, d5_dropout, d6_nodes, d6_activation, d6_dropout, lr, epochs)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRMSprop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0000001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;31m#     score = [model.evaluate(X_valid,Y_valid)[1], model.evaluate(X_test,Y_test)[1]]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 963\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    964\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1705\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1235\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1236\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2478\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2479\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, Flatten\n",
    "\n",
    "def steks(d1_nodes=2048,\n",
    "          d1_activation='relu',\n",
    "          d1_dropout=.2,\n",
    "          d2_nodes=1024,\n",
    "          d2_activation='relu',\n",
    "          d2_dropout=.2,\n",
    "          d3_nodes=512,\n",
    "          d3_activation='relu',\n",
    "          d3_dropout=.2,\n",
    "          d4_nodes=256,\n",
    "          d4_activation='relu',\n",
    "          d4_dropout=.2,\n",
    "          d5_nodes=128,\n",
    "          d5_activation='relu',\n",
    "          d5_dropout=.2,\n",
    "          d6_nodes=64,\n",
    "          d6_activation='relu',\n",
    "          d6_dropout=.2,\n",
    "          lr=0.0006,\n",
    "          epochs=50):\n",
    "    \n",
    "    # create model\n",
    "#     model = Sequential()\n",
    "#     model.add(Embedding(em_input_dim,em_nodes,input_length=313))\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(d1_nodes, input_shape=(313,),activation=d1_activation))\n",
    "#     model.add(Dropout(.2))\n",
    "#     model.add(Dense(d2_nodes, activation=d2_activation))\n",
    "#     model.add(Dropout(.2))\n",
    "#     model.add(Dense(output_dim, activation='softmax'))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(d1_nodes, input_shape=(num_words,),activation=d1_activation))\n",
    "    model.add(Dropout(d1_dropout))\n",
    "#     model.add(Embedding(em_input_dim,em_nodes,input_length=200))\n",
    "#     model.add(Flatten())\n",
    "    model.add(Dense(d1_nodes, activation=d1_activation))\n",
    "    model.add(Dropout(d1_dropout))\n",
    "    model.add(Dense(d2_nodes, activation=d2_activation))\n",
    "    model.add(Dropout(d2_dropout))\n",
    "    model.add(Dense(d3_nodes, activation=d3_activation))\n",
    "    model.add(Dropout(d3_dropout))\n",
    "    model.add(Dense(d4_nodes, activation=d4_activation))\n",
    "    model.add(Dropout(d4_dropout))\n",
    "    model.add(Dense(d5_nodes, activation=d5_activation))\n",
    "    model.add(Dropout(d5_dropout))\n",
    "    model.add(Dense(d6_nodes, activation=d6_activation))\n",
    "    model.add(Dropout(d6_dropout))\n",
    "    \n",
    "    model.add(Dense(output_dim, activation='softmax'))\n",
    "\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.RMSprop(lr=lr,decay=0.0000001), metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X_train, Y_train, epochs=epochs, batch_size=32, validation_data=(X_valid,Y_valid),shuffle=True)\n",
    "#     score = [model.evaluate(X_valid,Y_valid)[1], model.evaluate(X_test,Y_test)[1]]\n",
    "    score = model.evaluate(X_test,Y_test)[1]\n",
    "    return score\n",
    "\n",
    "scores = []\n",
    "# for dn in [200,150]:\n",
    "#     for dt in [150,100]:\n",
    "#         for dh in [100,50]:\n",
    "#             scores.append(('d1_nodes={}, d2_nodes={}, d3_nodes={}'.format(dn,dt,dh),steks(d4_nodes=dn,d5_nodes=dt,d6_nodes=dh)))\n",
    "# for i in [0.0006,0.00056,0.00036]:\n",
    "#     for e in [50,25,75]:\n",
    "#         scores.append(('dropout = {}'.format(i),steks(lr=i,epochs=e)))\n",
    "scores.append(steks())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "fails = 0\n",
    "successes = 0\n",
    "\n",
    "for row in df[313:].itertuples():\n",
    "    idx = row.Index\n",
    "    actual = df.at[idx,'c2']\n",
    "    pred = model.evaluate(tokenz.texts_to_matrix(row.desc))[0]\n",
    "    pred = pred.argmax(axis=-1)\n",
    "    if actual != pred:\n",
    "        fails += 1\n",
    "    elif actual == pred:\n",
    "        successes += 1\n",
    "        \n",
    "    \n",
    "print('Accuracy: {}'.format(round(successes/len(df[313:])*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KCbXKt627K7z"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "import pandas as pd\n",
    "\n",
    "def data():\n",
    "    \n",
    "    def c_str(strings, clean_list = []):\n",
    "        '''\n",
    "        Purpose: Clean string of punct and unwanted words, then split into lists\n",
    "\n",
    "        :type text str\n",
    "        :type clean_list list\n",
    "        ********\n",
    "        clean_list: optional arg\n",
    "        Accepts passed list of string objs to be removed from string\n",
    "        '''\n",
    "        csstr = []\n",
    "        dDesc = strings\n",
    "        dDesc = dDesc.str.lower()\n",
    "        replace_re = '|'.join([re.escape(s) for s in clean_list] + [r'\\s+', r'[0-9]+'])\n",
    "        for idx, str in enumerate(dDesc):\n",
    "            cstr = []\n",
    "            str = re.sub(r'\\W+', ' ', str)\n",
    "            if str.startswith('uber'):\n",
    "                str = 'uber'\n",
    "            str = str.split()\n",
    "            for word in str:\n",
    "                cstr += [re.sub(replace_re, '', word)]\n",
    "            str = cstr\n",
    "            str = ' '.join([word for word in str if word])\n",
    "            dDesc[idx] = str\n",
    "        return dDesc\n",
    "    \n",
    "    df = pd.read_csv('./data/Budget - Sheet14.csv')\n",
    "    clean_list = ['phoenix','avondale','tempe', 'help', 'glendale','az','ny']\n",
    "    df.loc[df.c2 == 'B1','c2'] = 'B'\n",
    "\n",
    "    df.desc = c_str(df.desc, clean_list)\n",
    "    df.to_pickle('./data/Trans_04-27.pkl')\n",
    "    df.c2.value_counts()\n",
    "    tdf = df.copy().drop('price',axis=1)\n",
    "                        \n",
    "    tokenz = Tokenizer(num_words=180)\n",
    "    tokenz.fit_on_texts(df.desc)\n",
    "    labels, d_classes = pd.factorize(df.c2)\n",
    "    input_dim = len(tokenz.word_index)\n",
    "    encoded = tokenz.texts_to_matrix(df.desc)\n",
    "\n",
    "    labels = to_categorical(labels)\n",
    "\n",
    "    output_dim = len(d_classes)\n",
    "\n",
    "    train_perc = .7\n",
    "    le = len(encoded)\n",
    "    t_end = int(le * train_perc)\n",
    "    v_end = int(t_end + (le - t_end) * .5)\n",
    "\n",
    "    X_train = encoded[:t_end]\n",
    "    X_valid = encoded[t_end:v_end]\n",
    "    X_test = encoded[v_end:]\n",
    "\n",
    "    Y_train = labels[:t_end]\n",
    "    Y_valid = labels[t_end:v_end]\n",
    "    Y_test = labels[v_end:]\n",
    "\n",
    "    return X_train, X_valid, X_test, Y_train, Y_valid, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, Flatten\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "\n",
    "def model(X_train, X_valid, X_test, Y_train, Y_valid, Y_test):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1024, input_shape=(180,),activation={{choice(['relu','sigmoid','tanh'])}}))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    model.add(Dense({{choice([512, 1024, 256])}}, activation={{choice(['relu','sigmoid','tanh'])}}))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    model.add(Dense({{choice([256, 1024, 64])}}, activation={{choice(['relu','sigmoid','tanh'])}}))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    model.add(Dense({{choice([128, 256, 512])}}, activation={{choice(['relu','sigmoid','tanh'])}}))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    model.add(Dense({{choice([64, 256, 512])}}, activation={{choice(['relu','sigmoid','tanh'])}}))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    model.add(Dense({{choice([32, 64, 128])}}, activation={{choice(['relu','sigmoid','tanh'])}}))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    model.add(Dense({{choice([16, 32, 64])}}, activation={{choice(['relu','sigmoid','tanh'])}}))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    \n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=keras.optimizers.RMSprop(lr={{uniform(0.00003, 0.00006)}},decay=0.0000001),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X_train,\n",
    "              Y_train,\n",
    "              epochs=epochs,\n",
    "              batch_size=32,\n",
    "              validation_data=(X_valid,Y_valid),\n",
    "              shuffle=True)\n",
    "    \n",
    "    score, acc = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    \n",
    "    print('Test accuracy:', acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import re\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import keras\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense, Dropout, Embedding, Flatten\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.preprocessing.text import Tokenizer\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils import to_categorical\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import keras\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense, Dropout, Embedding, Flatten\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, conditional\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import networkx as n\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'activation': hp.choice('activation', ['relu','sigmoid','tanh']),\n",
      "        'Dropout': hp.uniform('Dropout', 0, 1),\n",
      "        'Dense': hp.choice('Dense', [512, 1024, 256]),\n",
      "        'activation_1': hp.choice('activation_1', ['relu','sigmoid','tanh']),\n",
      "        'Dropout_1': hp.uniform('Dropout_1', 0, 1),\n",
      "        'Dense_1': hp.choice('Dense_1', [256, 1024, 64]),\n",
      "        'activation_2': hp.choice('activation_2', ['relu','sigmoid','tanh']),\n",
      "        'Dropout_2': hp.uniform('Dropout_2', 0, 1),\n",
      "        'Dense_2': hp.choice('Dense_2', [128, 256, 512]),\n",
      "        'activation_3': hp.choice('activation_3', ['relu','sigmoid','tanh']),\n",
      "        'Dropout_3': hp.uniform('Dropout_3', 0, 1),\n",
      "        'Dense_3': hp.choice('Dense_3', [64, 256, 512]),\n",
      "        'activation_4': hp.choice('activation_4', ['relu','sigmoid','tanh']),\n",
      "        'Dropout_4': hp.uniform('Dropout_4', 0, 1),\n",
      "        'Dense_4': hp.choice('Dense_4', [32, 64, 128]),\n",
      "        'activation_5': hp.choice('activation_5', ['relu','sigmoid','tanh']),\n",
      "        'Dropout_5': hp.uniform('Dropout_5', 0, 1),\n",
      "        'Dense_5': hp.choice('Dense_5', [16, 32, 64]),\n",
      "        'activation_6': hp.choice('activation_6', ['relu','sigmoid','tanh']),\n",
      "        'Dropout_6': hp.uniform('Dropout_6', 0, 1),\n",
      "        'lr': hp.uniform('lr', 0.00003, 0.00006),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "   1: \n",
      "   2: \n",
      "   3: def c_str(strings, clean_list = []):\n",
      "   4:     '''\n",
      "   5:     Purpose: Clean string of punct and unwanted words, then split into lists\n",
      "   6: \n",
      "   7:     :type text str\n",
      "   8:     :type clean_list list\n",
      "   9:     ********\n",
      "  10:     clean_list: optional arg\n",
      "  11:     Accepts passed list of string objs to be removed from string\n",
      "  12:     '''\n",
      "  13:     csstr = []\n",
      "  14:     dDesc = strings\n",
      "  15:     dDesc = dDesc.str.lower()\n",
      "  16:     replace_re = '|'.join([re.escape(s) for s in clean_list] + [r'\\s+', r'[0-9]+'])\n",
      "  17:     for idx, str in enumerate(dDesc):\n",
      "  18:         cstr = []\n",
      "  19:         str = re.sub(r'\\W+', ' ', str)\n",
      "  20:         if str.startswith('uber'):\n",
      "  21:             str = 'uber'\n",
      "  22:         str = str.split()\n",
      "  23:         for word in str:\n",
      "  24:             cstr += [re.sub(replace_re, '', word)]\n",
      "  25:         str = cstr\n",
      "  26:         str = ' '.join([word for word in str if word])\n",
      "  27:         dDesc[idx] = str\n",
      "  28:     return dDesc\n",
      "  29: \n",
      "  30: df = pd.read_csv('./data/Budget - Sheet14.csv')\n",
      "  31: clean_list = ['phoenix','avondale','tempe', 'help', 'glendale','az','ny']\n",
      "  32: df.loc[df.c2 == 'B1','c2'] = 'B'\n",
      "  33: \n",
      "  34: df.desc = c_str(df.desc, clean_list)\n",
      "  35: df.to_pickle('./data/Trans_04-27.pkl')\n",
      "  36: df.c2.value_counts()\n",
      "  37: tdf = df.copy().drop('price',axis=1)\n",
      "  38:                     \n",
      "  39: tokenz = Tokenizer(num_words=180)\n",
      "  40: tokenz.fit_on_texts(df.desc)\n",
      "  41: labels, d_classes = pd.factorize(df.c2)\n",
      "  42: input_dim = len(tokenz.word_index)\n",
      "  43: encoded = tokenz.texts_to_matrix(df.desc)\n",
      "  44: \n",
      "  45: labels = to_categorical(labels)\n",
      "  46: \n",
      "  47: output_dim = len(d_classes)\n",
      "  48: \n",
      "  49: train_perc = .7\n",
      "  50: le = len(encoded)\n",
      "  51: t_end = int(le * train_perc)\n",
      "  52: v_end = int(t_end + (le - t_end) * .5)\n",
      "  53: \n",
      "  54: X_train = encoded[:t_end]\n",
      "  55: X_valid = encoded[t_end:v_end]\n",
      "  56: X_test = encoded[v_end:]\n",
      "  57: \n",
      "  58: Y_train = labels[:t_end]\n",
      "  59: Y_valid = labels[t_end:v_end]\n",
      "  60: Y_test = labels[v_end:]\n",
      "  61: \n",
      "  62: \n",
      "  63: \n",
      "  64: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     model = Sequential()\n",
      "   4:     model.add(Dense(1024, input_shape=(180,),activation=space['activation']))\n",
      "   5:     model.add(Dropout(space['Dropout']))\n",
      "   6:     model.add(Dense(space['Dense'], activation=space['activation_1']))\n",
      "   7:     model.add(Dropout(space['Dropout_1']))\n",
      "   8:     model.add(Dense(space['Dense_1'], activation=space['activation_2']))\n",
      "   9:     model.add(Dropout(space['Dropout_2']))\n",
      "  10:     model.add(Dense(space['Dense_2'], activation=space['activation_3']))\n",
      "  11:     model.add(Dropout(space['Dropout_3']))\n",
      "  12:     model.add(Dense(space['Dense_3'], activation=space['activation_4']))\n",
      "  13:     model.add(Dropout(space['Dropout_4']))\n",
      "  14:     model.add(Dense(space['Dense_4'], activation=space['activation_5']))\n",
      "  15:     model.add(Dropout(space['Dropout_5']))\n",
      "  16:     model.add(Dense(space['Dense_5'], activation=space['activation_6']))\n",
      "  17:     model.add(Dropout(space['Dropout_6']))\n",
      "  18:     \n",
      "  19:     model.add(Dense(2, activation='softmax'))\n",
      "  20: \n",
      "  21: \n",
      "  22:     # Compile model\n",
      "  23:     model.compile(loss='categorical_crossentropy',\n",
      "  24:                   optimizer=keras.optimizers.RMSprop(lr=space['lr'],decay=0.0000001),\n",
      "  25:                   metrics=['accuracy'])\n",
      "  26: \n",
      "  27:     model.fit(X_train,\n",
      "  28:               Y_train,\n",
      "  29:               epochs=epochs,\n",
      "  30:               batch_size=32,\n",
      "  31:               validation_data=(X_valid,Y_valid),\n",
      "  32:               shuffle=True)\n",
      "  33:     \n",
      "  34:     score, acc = model.evaluate(X_test, Y_test, verbose=0)\n",
      "  35:     \n",
      "  36:     print('Test accuracy:', acc)\n",
      "  37:     return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
      "  38: \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'd1_nodes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-8019bcc16b4b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m                                           \u001b[0mmax_evals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                                           \u001b[0mtrials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrials\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                                           notebook_name='Transaction_Desc_Classifier')\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Evalutation of best performing model:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\hyperas\\optim.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(model, data, algo, max_evals, trials, functions, rseed, notebook_name, verbose, eval_space, return_space)\u001b[0m\n\u001b[0;32m     65\u001b[0m                                      \u001b[0mfull_model_string\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                                      \u001b[0mnotebook_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnotebook_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m                                      verbose=verbose)\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0mbest_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\hyperas\\optim.py\u001b[0m in \u001b[0;36mbase_minimizer\u001b[1;34m(model, data, functions, algo, max_evals, trials, rseed, full_model_string, notebook_name, verbose, stack)\u001b[0m\n\u001b[0;32m    131\u001b[0m              \u001b[0mtrials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m              \u001b[0mrstate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m              return_argmin=True),\n\u001b[0m\u001b[0;32m    134\u001b[0m         \u001b[0mget_space\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m     )\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin)\u001b[0m\n\u001b[0;32m    305\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 307\u001b[1;33m             \u001b[0mreturn_argmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m         )\n\u001b[0;32m    309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\hyperopt\\base.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(self, fn, space, algo, max_evals, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin)\u001b[0m\n\u001b[0;32m    633\u001b[0m             \u001b[0mpass_expr_memo_ctrl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpass_expr_memo_ctrl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 635\u001b[1;33m             return_argmin=return_argmin)\n\u001b[0m\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin)\u001b[0m\n\u001b[0;32m    318\u001b[0m                     verbose=verbose)\n\u001b[0;32m    319\u001b[0m     \u001b[0mrval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m     \u001b[0mrval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    321\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, N, block_until_done)\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m                 \u001b[1;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstopped\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m     90\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m                     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'job exception: %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\hyperopt\\base.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[0;32m    838\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[1;32m--> 840\u001b[1;33m             \u001b[0mrval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    841\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PythPro\\Banker\\temp_model.py\u001b[0m in \u001b[0;36mkeras_fmin_fnct\u001b[1;34m(space)\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'd1_nodes' is not defined"
     ]
    }
   ],
   "source": [
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform, conditional\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    best_run, best_model = optim.minimize(model=model,\n",
    "                                          data=data,\n",
    "                                          algo=tpe.suggest,\n",
    "                                          max_evals=20,\n",
    "                                          trials=Trials(),\n",
    "                                          notebook_name='Transaction_Desc_Classifier')\n",
    "    X_train, Y_train, X_test, Y_test = data()\n",
    "    print(\"Evalutation of best performing model:\")\n",
    "    print(best_model.evaluate(X_test, Y_test))\n",
    "    print(\"Best performing model chosen hyper-parameters:\")\n",
    "    print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11\n"
     ]
    }
   ],
   "source": [
    "import networkx as n\n",
    "print(n.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "Transaction Desc Classifier.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
