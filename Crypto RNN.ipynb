{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.youtube.com/watch?v=ne-dpRdNReI\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn import preprocessing\n",
    "from collections import deque, OrderedDict\n",
    "import random\n",
    "from old.fastai.dataset import *\n",
    "from old.fastai.column_data import *\n",
    "from fastai.metrics import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path(\"../data/crypto_data/\")\n",
    "sets = [\"BTC-USD\", \"BCH-USD\", \"ETH-USD\", \"LTC-USD\"]\n",
    "df_s = [pd.read_csv(PATH / f\"{s}.csv\", names = [\"time\",\"low\",\"high\",\"open\",f\"{s}_close\",f\"{s}_volume\"],index_col=\"time\").drop([\"low\",\"high\",\"open\"],axis=1) for s in sets]\n",
    "df_m = pd.concat(df_s, axis=1, join=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 30 #transactions prior to prediction point\n",
    "pred_period = 3 #minutes ahead to predict \n",
    "pred_ratio = \"LTC-USD\"\n",
    "val_pct = .05\n",
    "bs = 25\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(current, future):\n",
    "    if float(future) > float(current):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df_m[\"future\"] = df_m[f\"{pred_ratio}_close\"].shift(-pred_period)\n",
    "df_m[\"future\"].fillna(method=\"ffill\",inplace=True)\n",
    "df_m[\"target\"] = list(map(classify,df_m[f\"{pred_ratio}_close\"],df_m[\"future\"]))\n",
    "df_m.sort_index(inplace=True)\n",
    "val_idx = int(len(df_m) * val_pct)\n",
    "df_v = df_m.iloc[-val_idx:].copy()\n",
    "df_t = df_m.iloc[:-val_idx].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_df(df):\n",
    "    df = df.copy().drop(\"future\",1)\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if col != \"target\":\n",
    "            df[col] = df[col].pct_change()\n",
    "            df.dropna(inplace=True)\n",
    "            df[col] = preprocessing.scale(df[col].values)\n",
    "    sequential_data = []\n",
    "    prev_period = deque(maxlen=seq_len)\n",
    "    buys = []\n",
    "    sells = []\n",
    "    for i in df.values:\n",
    "        prev_period.append([n for n in i[:-1]])\n",
    "        if len(prev_period) == seq_len:\n",
    "#             sequential_data.append([np.array(prev_period), i[-1]])\n",
    "            if i[-1] == 0:\n",
    "                sells.append([np.array(prev_period),np.array(i[-1])])\n",
    "            elif i[-1] == 1:\n",
    "                buys.append([np.array(prev_period),np.array(i[-1])])\n",
    "    print(len(buys),len(sells))\n",
    "    lower = min(len(buys), len(sells))\n",
    "    buys = buys[:lower-1]\n",
    "    sells = sells[:lower-1]\n",
    "    balanced = buys+sells\n",
    "    random.shuffle(balanced)\n",
    "    data = OrderedDict()\n",
    "    labels = OrderedDict()\n",
    "    for i, (seq, targ) in enumerate(balanced):\n",
    "        data[i] = torch.FloatTensor(seq)\n",
    "        labels[i] =torch.FloatTensor(targ)\n",
    "    print(f\"Total: {len(balanced)}, Buys: {len(buys)}, Sells: {len(sells)}\")\n",
    "    \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35161 46614\n",
      "Total: 70320, Buys: 35160, Sells: 35160\n",
      "1799 2469\n",
      "Total: 3596, Buys: 1798, Sells: 1798\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y = prep_df(df_t)\n",
    "val_x, val_y = prep_df(df_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx].float(), self.labels[idx].float()\n",
    "    \n",
    "trn_ds = Dataset(train_x, train_y)\n",
    "val_ds = Dataset(val_x, val_y)\n",
    "\n",
    "trn_dl = data.DataLoader(trn_ds, batch_size=bs)\n",
    "val_dl = data.DataLoader(val_ds, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstmclass(nn.Module):\n",
    "    def __init__(self,n_layers,h_dim):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.h_dim = h_dim\n",
    "        self.lstm = nn.LSTM(8, h_dim, n_layers, batch_first=True)\n",
    "        self.lin1 = nn.Linear(h_dim, h_dim//2)\n",
    "#         self.bn = nn.BatchNorm1d(seq_len)\n",
    "        self.drop = nn.Dropout(.2)\n",
    "        self.lin2 = nn.Linear(h_dim//2, h_dim//4)\n",
    "        self.drop2 = nn.Dropout(.1)\n",
    "        self.lin3 = nn.Linear(h_dim//4,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        hidden = self._init_hidden(x)\n",
    "#         print(\"x:\",x.shape, \"\\nHidden:\",hidden[0].shape)\n",
    "        self.lstm.flatten_parameters()\n",
    "        x, h = self.lstm(x, hidden)\n",
    "        x = self.lin1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.drop2(F.relu(self.lin2(x)))\n",
    "        x = F.relu(self.lin3(x))\n",
    "        return x.squeeze()\n",
    "    \n",
    "    def _init_hidden(self,x):\n",
    "        return (Variable(torch.zeros(self.n_layers, x.size(0), self.h_dim)),\n",
    "                Variable(torch.zeros(self.n_layers, x.size(0), self.h_dim)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self,h_dim,h_layers):\n",
    "        super().__init__()\n",
    "        self.h_dim = h_dim\n",
    "        self.h_layers = h_layers\n",
    "        self.lstm = nn.LSTM(8,h_dim,h_layers,batch_first=True)\n",
    "        self.lin1 = nn.Linear(h_dim, h_dim//2)\n",
    "        self.bn = nn.BatchNorm1d(seq_len)\n",
    "        self.drop = nn.Dropout(.1)\n",
    "        self.lin2 = nn.Linear(h_dim//2, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        hidden = Variable(torch.zeros(2,self.h_layers, x.size(0), self.h_dim).cuda())\n",
    "        print(hidden.shape)\n",
    "        self.lstm.flatten_parameters()\n",
    "        return x\n",
    "        x, h = self.lstm(x,hidden)\n",
    "        x = x.contiguous().view(-1,np.prod(x.size()[1:]))\n",
    "        x = self.lin1(x)\n",
    "        x = F.relu(self.bn(x))\n",
    "        x = self.drop(x)\n",
    "#         x = x.view(bs,-1)\n",
    "        x = F.softmax(self.lin2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_stats(ep_vals, epoch, values, decimals=6):\n",
    "    ep_vals[epoch]=list(np.round(values, decimals))\n",
    "    return ep_vals\n",
    "\n",
    "def print_stats(epoch, values, decimals=3):\n",
    "    layout = \"{!s:^10}\" + \" {!s:10}\" * len(values)\n",
    "    values = [epoch] + list(np.round(values, decimals))\n",
    "    print(layout.format(*values))\n",
    "\n",
    "names = [\"ep / it\",\"trn_loss\",\"val_loss\",\"accuracy\"]\n",
    "layout = \"{!s:10} \" * len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = LSTM(128,1)\n",
    "net = lstmclass(2,128)\n",
    "# net = net.cuda()\n",
    "lr = 2e-3\n",
    "optim = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep / it    trn_loss   val_loss   accuracy   \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-db0b44d8a61d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0mv_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_ys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0mv_ls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0maccs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_ys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mvals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mprint_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{epoch+1} / {i+1}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "t_ls = []\n",
    "v_ls = []\n",
    "accs = []\n",
    "ep_vals = OrderedDict()\n",
    "for epoch in range(epochs):\n",
    "    if epoch == 0: print(layout.format(*names))\n",
    "    val_it = iter(val_dl)\n",
    "    for i, (batch, ys) in enumerate(trn_dl, 0):\n",
    "        net.train()\n",
    "        optim.zero_grad()\n",
    "        pred = net(batch)\n",
    "        ls = loss(pred,ys.long())\n",
    "        t_ls.append(ls.item())\n",
    "        ls.backward()\n",
    "        optim.step()\n",
    "        vals = [np.mean(t_ls[-100:])]\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            net.eval()\n",
    "            with torch.no_grad():\n",
    "                v_bat, v_ys = next(val_it)\n",
    "                v = net(v_bat)\n",
    "                v_loss = loss(v, v_ys.long())\n",
    "                v_ls.append(v_loss.item())\n",
    "            accs.append(accuracy(v, v_ys.long().item()))\n",
    "            vals.extend((v_loss.item(), np.mean(accs[-3:])))\n",
    "            print_stats(f\"{epoch+1} / {i+1}\",vals)\n",
    "    ep_vals = append_stats(ep_vals, epoch+1, vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lstm2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-0e460541e0e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m save_checkpoint({\n\u001b[1;32m      7\u001b[0m             \u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0;34m'arch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlstm2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;34m'last_acc'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0maccs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lstm2' is not defined"
     ]
    }
   ],
   "source": [
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, PATH / filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')\n",
    "        \n",
    "save_checkpoint({\n",
    "            'epoch': epochs,\n",
    "            'arch': lstm2,\n",
    "            'state_dict': net.state_dict(),\n",
    "            'last_acc': accs[-1],\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "        }, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_bat, v_ys = next(val_it)\n",
    "v = net(v_bat)\n",
    "# accuracy(v,v_ys.long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 30])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
